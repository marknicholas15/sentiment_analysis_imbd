{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "standard-ozone",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "def pre_process_data(filepath):\n",
    "    positive_path = os.path.join(filepath, 'pos')\n",
    "    negative_path = os.path.join(filepath, 'neg')\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "    dataset = []\n",
    "    \n",
    "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
    "        with open(filename, 'r', errors='ignore') as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "    \n",
    "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
    "        with open(filename, 'r', errors='ignore') as f:\n",
    "            dataset.append((neg_label, f.read()))\n",
    "    \n",
    "    shuffle(dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "confidential-assets",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'When I first heard about the show, I heard a lot about it, and it was getting some good reviews. I watched the first episode of this \"forensic fairy tale\", as it so proclaims itself, and I really got hooked on it. I have loved it since. This show has a good sense of humour and it\\'s fun to see a good show like this. The cast is excellent as their characters, and I wouldn\\'t want to change them in any way.<br /><br />For those unfamiliar with this show, Pushing Daisies centers around a man named Ned (aka The Pie Maker, played by Lee Pace) who discovered a special gift when he was a boy: He could bring the dead back to life with the touch of a finger. He first did so with his dog, Digby. However, there is the catch: If he keeps a dead person alive for more than one minute, someone else dies. He learned this when he brought his mother back to life, and his childhood crush\\'s father died in Ned\\'s mother\\'s place. The other catch is if he touches the person again, they\\'re dead again, but this time for good. He learned this when his mother kissed him goodnight. His father took him to boarding school, and when he left, Ned never saw his father again.<br /><br />Almost 20 years later, Ned owns a pie bakery, cleverly titled \"The Pie Hole.\" A co-worker of Ned\\'s, Olive Snook (Kristin Chenoweth) has a crush on Ned, but Ned rejects her moves, trying not to get close to anyone, learning from past experiences. Private Investigator Emerson Cod (Chi McBride) discovered the gift that Ned has, and decides to make him a partner in solving murders. Ned touches the victim, asks who killed them, and when the minute is up, he touches them again, and they solve it. That\\'s how they usually solve it. Throughout the episodes, the murders have very interesting plots and be what people least expect.<br /><br />One day, Ned discovers that his next murder to solve is his childhood sweetheart, Charlotte \"Chuck\" Charles (Anna Friel). He brings her back to life and decides to break the rules and keep her alive. In her place, the funeral director, who stole jewelery from the corpses, died. When Emerson finds out, and when Chuck wants to help with solving the murders, he doesn\\'t agree a bit--for a while, we hear him call Chuck \\'Dead girl\\'. This is all kept in secret from Olive, Chuck\\'s aunts Vivian and Lily (Ellen Greene and Swoosie Kurtz, respectively), and everyone else for that matter, in case anyone recognized her from obituaries, the news, etc. Vivian and Lily, formerly synchronized swimmers, hadn\\'t left the house in years. Emerson, Ned, and Chuck agree to work together. Ned and Chuck grow to love each other, though they can\\'t touch each other ever again.<br /><br />This show is funny, has terrific characters, contains great plot twists, and will definitely get your spirits up. I hope it doesn\\'t get cancelled at 13 episodes.')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pre_process_data('train')\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "removed-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(\\\n",
    "                'C:/Users/markn/Artificial_Intelligence/NLP/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fallen-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            \n",
    "            except KeyError:\n",
    "                pass\n",
    "        \n",
    "        vectorized_data.append(sample_vecs)\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "forward-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_expected(dataset):\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fitting-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "basic-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_data, \\\n",
    "                                                   expected, test_size=0.2,\\\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "scientific-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "standing-decrease",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    new_data = []\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "    \n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        \n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "refined-green",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chicken-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = pad_trunc(X_train, maxlen)\n",
    "X_test = pad_trunc(X_test, maxlen)\n",
    "\n",
    "X_train = np.reshape(X_train, (len(X_train), maxlen, embedding_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rental-binding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 400, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "narrow-education",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eleven-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "X_test = np.reshape(X_test, (len(X_test), maxlen, embedding_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "downtown-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "played-judgment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "after-receipt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model..\n"
     ]
    }
   ],
   "source": [
    "print('Build model..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "standing-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D\n",
    "model2 = Sequential()\n",
    "model2.add(Conv1D(\n",
    "    filters,\n",
    "    kernel_size,\n",
    "    padding='valid',\n",
    "    activation='relu',\n",
    "    strides=1,\n",
    "    input_shape=(maxlen, embedding_dims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "temporal-mortgage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 398, 250)          225250    \n",
      "=================================================================\n",
      "Total params: 225,250\n",
      "Trainable params: 225,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "hispanic-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "circular-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(GlobalAveragePooling1D())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pending-westminster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 398, 250)          225250    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 250)               0         \n",
      "=================================================================\n",
      "Total params: 225,250\n",
      "Trainable params: 225,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "alert-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "coordinated-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(Dense(hidden_dims))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "empirical-warrant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 398, 250)          225250    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "=================================================================\n",
      "Total params: 288,000\n",
      "Trainable params: 288,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "younger-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(Dense(1))\n",
    "model2.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bronze-worthy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 398, 250)          225250    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 288,251\n",
      "Trainable params: 288,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "swedish-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "developing-stuff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 117s 6ms/step - loss: 0.4666 - accuracy: 0.7901 - val_loss: 0.3842 - val_accuracy: 0.8330\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 116s 6ms/step - loss: 0.3449 - accuracy: 0.8553 - val_loss: 0.3135 - val_accuracy: 0.8598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x28a0a501c48>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train, \n",
    "          batch_size= batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aboriginal-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model2.to_json()\n",
    "with open(\"cnn_model.json\", 'w') as json_file:\n",
    "    json_file.write(model_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "consolidated-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save_weights(\"cnn_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "front-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "differential-dividend",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "herbal-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cnn_model.json\", 'r') as json_file:\n",
    "    json_string = json_file.read()\n",
    "model = model_from_json(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "corrected-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('cnn_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "closing-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = \"\"\"I hate that the dismal weather had me down for so long,\n",
    "when will it break! Ugh, when does happiness return? The sun is blinding\n",
    "and the puffy clouds are too thin. I can't wait for the weekend.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "golden-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_list = tokenize_and_vectorize([(1, sample1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aggressive-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec_list = pad_trunc(vec_list, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bronze-mainstream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "transsexual-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen,\\\n",
    "                                      embedding_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "competent-think",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23705384]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "internal-selling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-timber",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
